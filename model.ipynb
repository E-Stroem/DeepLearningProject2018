{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "Anders Melander, s144277 <br>\n",
    "Sophie Silberbrandt, s144246 <br>\n",
    "Emil Str√∏m, s144259 <br>\n",
    "\n",
    "In collaboration with Corti.ai. https://corti.ai\n",
    "\n",
    "02456 - Deep Learning - DTU Compute Autumn 2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "Many of the following commands have been done on a Linux machine, adjust accordingly to work on your device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data\n",
    "The data can be downloaded and extracted automatically using the cell below.\n",
    "\n",
    "If you need to do it manually download the data from https://drive.google.com/uc?id=1Hnlqn48mGNfEbSOFaRcTGkUOsU0SHL5V and extract the it to `an4_dataset/` in your working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "AKOw_xuILYOE",
    "outputId": "4455c3c4-ef31-48c1-8d78-263fb1639168"
   },
   "outputs": [],
   "source": [
    "# DOWNLOAD DATA\n",
    "import os\n",
    "\n",
    "downLoadAutomatically = True\n",
    "\n",
    "if downloadAutomatically:\n",
    "    if not (os.path.isdir(\"zippedData/\") and os.path.isdir(\"an4_dataset/\")):\n",
    "        !mkdir zippedData/\n",
    "        !wget 'https://drive.google.com/uc?id=1Hnlqn48mGNfEbSOFaRcTGkUOsU0SHL5V' \\\n",
    "        -O zippedData/an4_dataset.zip\n",
    "        !unzip -q zippedData/an4_dataset.zip\n",
    "        print(\"-------------DATA UNZIPPED TO an4_dataset/-------------\")\n",
    "    else:\n",
    "        print(\"DATA ALREADY DOWNLOADED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the needed modules.\n",
    "The modules required for the code to run is:<br>\n",
    "- numpy\n",
    "- torch\n",
    "- librosa\n",
    "\n",
    "Uncomment the lines as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bTuUA77Q_9H0"
   },
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install torch\n",
    "#!pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch bindings for Warp-ctc\n",
    "The loss function used is the warp-CTC implementation done by Sean Naren, see GitHub: <br>\n",
    "https://github.com/SeanNaren/warp-ctc \n",
    "\n",
    "\n",
    "The commands below will install warp-ctc on Linux.\n",
    "To install set `build_warpCTC = True` and run the cell. After install, set back to `False` and restart the kernel for python to recognize the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_warpCTC = False\n",
    "if build_warpCTC:\n",
    "    !git clone https://github.com/SeanNaren/warp-ctc.git\n",
    "    !cd warp-ctc; mkdir build; cd build; cmake ..; make\n",
    "    !cd warp-ctc/pytorch_binding; python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from warpctc_pytorch import CTCLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2spectrogram(path_to_file, sampling_ms=25, hop_ms=10, num_mels=13):\n",
    "    y, sr = librosa.load(path_to_file, sr=None)\n",
    "    n_fft = int((sampling_ms / 1000) * sr)\n",
    "    hop_length = int((hop_ms / 1000) * sr)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=num_mels)\n",
    "    spectrogram = spectrogram.astype(np.float32)\n",
    "\n",
    "    mean, std = np.mean(spectrogram), np.std(spectrogram)\n",
    "    spectrogram = (spectrogram - mean) / std\n",
    "\n",
    "    return spectrogram\n",
    "\n",
    "\n",
    "def wav2npy(path_to_data, num_mels):\n",
    "    for f in os.listdir(path_to_data):\n",
    "        if '.wav' in f:\n",
    "            filename = path_to_data + f\n",
    "            spec = wav2spectrogram(filename, num_mels=num_mels)\n",
    "            npToSave = np.array(spec)\n",
    "            outFilename = path_to_data + f.split('.')[0] + '.npy'\n",
    "            np.save(outFilename, npToSave)\n",
    "\n",
    "\n",
    "def load_npy(path_to_data):\n",
    "    data = []\n",
    "    for f in os.listdir(path_to_data):\n",
    "        if '.txt' in f:\n",
    "            ftxt = path_to_data + f\n",
    "            fnpy = path_to_data + f.split('.')[0] + '.npy'\n",
    "            with open(ftxt, 'r') as ftxtOpened:\n",
    "                txt = ftxtOpened.read()\n",
    "            data.append([np.load(fnpy), txt])\n",
    "\n",
    "    num_mels = len(data[0][0])\n",
    "    return np.array(data), num_mels\n",
    "\n",
    "\n",
    "def get_batch(batch_data, max_time):\n",
    "    batch_out = np.zeros(shape=(batch_data.shape[0], batch_data[0][0].shape[0], max_time), dtype=np.float32)\n",
    "    timesteps, labels, labelLens = [], [], []\n",
    "    for i, (batch, label) in enumerate(batch_data):\n",
    "        time = batch.shape[1]\n",
    "        batch_out[i, :, :time] = batch\n",
    "        timesteps.append(time // 2)\n",
    "        labels.append(label)\n",
    "        labelLens.append(len(label))\n",
    "\n",
    "    return batch_out, timesteps, \"\".join(labels), labelLens\n",
    "\n",
    "def WER(r, h):\n",
    "    r = r.split()\n",
    "    h = h.split()\n",
    "    d = np.zeros((len(r)+1)*(len(h)+1), dtype=np.uint8)\n",
    "    d = d.reshape((len(r)+1, len(h)+1))\n",
    "    for i in range(len(r)+1):\n",
    "        for j in range(len(h)+1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "    \n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                d[i][j] = d[i-1][j-1]\n",
    "            else:\n",
    "                substitution = d[i-1][j-1] + 1\n",
    "                insertion = d[i][j-1] + 1\n",
    "                deletion = d[i-1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "\n",
    "    return d[len(r)][len(h)]/len(r)\n",
    "\n",
    "\n",
    "def cleanPred(pred):\n",
    "    outString = \"\"\n",
    "    prevChar = None\n",
    "    for char in pred:\n",
    "        if char != '#':\n",
    "            if char != prevChar:\n",
    "                outString += char\n",
    "        prevChar = char\n",
    "    return outString\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 14460
    },
    "colab_type": "code",
    "id": "z8no6vyeCWnV",
    "outputId": "3a10adb3-9d46-4548-947d-0472ae7dc72d"
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"an4_dataset/train/\"\n",
    "VALIDATION_PATH = \"an4_dataset/validation/\"\n",
    "ALPHABET = '#ABCDEFGHIJKLMNOPQRSTUVWXYZ '\n",
    "\n",
    "\n",
    "# Create the spectograms\n",
    "NUM_MELS = 13\n",
    "MAX_SEQ_LEN = 700\n",
    "PREPROCESS = False\n",
    "if PREPROCESS:\n",
    "    wav2npy(TRAIN_PATH, NUM_MELS)\n",
    "    wav2npy(VALIDATION_PATH, NUM_MELS)\n",
    "\n",
    "train_data, num_mels = load_npy(TRAIN_PATH)\n",
    "validation_data, _ = load_npy(VALIDATION_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, alphabet, num_mels):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        IN_CHANNELS = num_mels\n",
    "        CONVR_CHANNELS = 250\n",
    "        KWR = 48\n",
    "        STRIDE = 2\n",
    "        CONVB_CHANNELS = 250\n",
    "        CONVBEnd_CHANNELS = 2000\n",
    "        KWB = 7\n",
    "        KWBEnd = 32\n",
    "        DROPOUT = 0.5\n",
    "\n",
    "        CONVG_CHANNELS = 2000\n",
    "        KWG = 1\n",
    "        N_VOC = len(alphabet)\n",
    "\n",
    "        # Red part\n",
    "        self.convr1 = nn.Conv1d(IN_CHANNELS, CONVR_CHANNELS, KWR, padding=23, stride=STRIDE)\n",
    "        self.dropr1 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        # Blue part\n",
    "        self.convb1 = nn.Conv1d(CONVR_CHANNELS, CONVB_CHANNELS, KWB, padding=3)\n",
    "        self.dropb1 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.convb2 = nn.Conv1d(CONVB_CHANNELS, CONVB_CHANNELS, KWB, padding=3)\n",
    "        self.dropb2 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.convb3 = nn.Conv1d(CONVB_CHANNELS, CONVB_CHANNELS, KWB, padding=3)\n",
    "        self.dropb3 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.convb4 = nn.Conv1d(CONVB_CHANNELS, CONVB_CHANNELS, KWB, padding=3)\n",
    "        self.dropb4 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.convb5 = nn.Conv1d(CONVB_CHANNELS, CONVB_CHANNELS, KWB, padding=3)\n",
    "        self.dropb5 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.convb6 = nn.Conv1d(CONVB_CHANNELS, CONVB_CHANNELS, KWB, padding=3)\n",
    "        self.dropb6 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.convb7 = nn.Conv1d(CONVB_CHANNELS, CONVB_CHANNELS, KWB, padding=3)\n",
    "        self.dropb7 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.padb8 = nn.ZeroPad2d((15, 16, 0, 0))\n",
    "        self.convb8 = nn.Conv1d(CONVB_CHANNELS, CONVBEnd_CHANNELS, KWBEnd)\n",
    "        self.dropb8 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        # Gray part\n",
    "        self.convg1 = nn.Conv1d(CONVBEnd_CHANNELS, CONVG_CHANNELS, KWG)\n",
    "        self.dropg1 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.convg2 = nn.Conv1d(CONVG_CHANNELS, N_VOC, KWG)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropr1(F.relu(self.convr1(x)))\n",
    "        x = self.dropb1(F.relu(self.convb1(x)))\n",
    "        x = self.dropb2(F.relu(self.convb2(x)))\n",
    "        x = self.dropb3(F.relu(self.convb3(x)))\n",
    "        x = self.dropb4(F.relu(self.convb4(x)))\n",
    "        x = self.dropb5(F.relu(self.convb5(x)))\n",
    "        x = self.dropb6(F.relu(self.convb6(x)))\n",
    "        x = self.dropb7(F.relu(self.convb7(x)))\n",
    "        x = self.dropb8(F.relu(self.convb8(self.padb8(x))))\n",
    "        x = self.dropg1(F.relu(self.convg1(x)))\n",
    "        x = self.convg2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "net = Net(ALPHABET, num_mels).cuda()\n",
    "print(net)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 300\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    np.random.shuffle(train_data)\n",
    "    training_loss = []\n",
    "    net.train()\n",
    "    for batch in range(len(train_data) // BATCH_SIZE):\n",
    "        optimizer.zero_grad()\n",
    "        batch_data = train_data[batch * BATCH_SIZE: (batch + 1) * BATCH_SIZE]\n",
    "        specs, n_timesteps, labels, labelLens = get_batch(batch_data, MAX_SEQ_LEN)\n",
    "\n",
    "        specInput = torch.from_numpy(specs).cuda()\n",
    "        targets = torch.IntTensor(list(map(ALPHABET.index, labels)))\n",
    "\n",
    "        # Permute output to have right shape for ctc_loss\n",
    "        input_lengths = torch.IntTensor(n_timesteps)\n",
    "        label_lengths = torch.IntTensor(labelLens)\n",
    "\n",
    "        logits = net(specInput).permute([2, 0, 1]).cuda()\n",
    "        loss = CTCLoss(size_average=True)(logits, targets, input_lengths, label_lengths)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += [loss.item()]\n",
    "    print(f'Epoch: {epoch + 1}, CTC loss: {np.mean(training_loss):1.4f}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "       \n",
    "    net.eval()\n",
    "    valLoss = 0\n",
    "    #Validation checks CTC loss and WER on elements not in training set\n",
    "    Val_WER = 0\n",
    "    for val in validation_data:\n",
    "        spectrogram = val[0]\n",
    "\n",
    "        spectrogramTensor = torch.from_numpy(spectrogram)\n",
    "\n",
    "        specInput = spectrogramTensor.unsqueeze(0).float().cuda()\n",
    "\n",
    "        target = torch.IntTensor(list(map(ALPHABET.index, val[1])))\n",
    "        \n",
    "        output = net(specInput)\n",
    "        outString = \"\"\n",
    "        for i in range(output.shape[2]):\n",
    "            outString += ALPHABET[int(torch.argmax(output[:,:,i]))]\n",
    "        Val_WER = Val_WER + WER(val[1], cleanPred(outString))\n",
    "        \n",
    "        \n",
    "        output = output.permute([2,0,1])\n",
    "        outshape = output.shape\n",
    "        \n",
    "        input_length = torch.IntTensor([outshape[0]])\n",
    "        target_length = torch.IntTensor([len(val[1])])\n",
    "        \n",
    "        \n",
    "\n",
    "        Loss = CTCLoss(size_average=True)(output, target, input_length, target_length)\n",
    "        valLoss += float(Loss.item())\n",
    "    print(f'Epoch: {epoch + 1}, Validation loss: {valLoss/len(validation_data):1.4f}')\n",
    "    print(f'Epoch: {epoch + 1}, WER: {Val_WER/len(validation_data):1.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HtiiqyUpPwUR"
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "\n",
    "for k in range(N):\n",
    "    data = validation_data[k]\n",
    "    spec = data[0]\n",
    "    net.eval()\n",
    "\n",
    "    specTensor = torch.from_numpy(spec)\n",
    "\n",
    "    spec2 = specTensor.unsqueeze(0).float()\n",
    "\n",
    "    spec2 = spec2.cuda()\n",
    "    output = net(spec2)\n",
    "\n",
    "    outString = \"\"\n",
    "    for i in range(output.shape[2]):\n",
    "        outString += ALPHABET[int(torch.argmax(output[:,:,i]))]\n",
    "\n",
    "    print('True:',data[1])\n",
    "    print('Pred:', outString)\n",
    "    print('Clean:', cleanPred(outString))\n",
    "    print('WER:', WER(data[1],cleanPred(outString)))\n",
    "    print('----------------------------')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Corti_warpctc",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
